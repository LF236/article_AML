AML es un campo que ha surgido en respuesta al uso de tecnicas de Aprendizaje Máquina en la industria. Este se introdujo en el año 2013 para constrarrestar las vulnerabilidades de las redes neuronales en vision artififial, sin embargo sus raices se remontan a clasificadores de spam en los 2000 (Frederickson et al. 2018) y a teorías de aprendizaje computacional en presencia de ruido malicioso en 1993 (Frederickson et al., 2018). AML se define de diversas maneras, desde llevar a cabo ataques contra sistemas basados en ML hasta examinar sus vulnerabilidades y proponer estrategias de defensa (Anthi et al., 2021; Lin & Biggio, 2021; Mehta et al., 2022; Ebrahimabadi et al., 2021; Mumcu et al., 2022). La literatura sobre AML aborda una variedad de aspectos, desde técnicas de ataque hasta medidas de mitigación, centradas en dominios específicos como procesamiento del lenguaje natural, redes vehiculares o ciberseguridad (Alsmadi et al., 2022; Qayyum et al., 2020; Martins et al., 2020; Maiorca et al., 2020; Olney & Karam, 2022). 

Consideraciones previas de Aprendizaje Máquina aplicadas a Aprendizaje Máquina Adversaria 

-Problemas del Aprendizaje Máquina

El Aprendizaje Máquina (ML) es una tecnología ampliamente utilizada en diversos campos, pero su susceptibilidad a fallos puede ser explotada por actores malintencionados, comprometiendo la integridad, confidencialidad o disponibilidad de los modelos o sistemas de ML (Mehta et al., 2022; Wilhjelm & Younis, 2020). 

No hay un algoritmo universalmente superior en ML y los adversarios están cada vez más interesados en atacar sistemas basados en ML, motivados por el robo de información, sabotaje u objetivos económicos (Anthi et al., 2021; Mehta et al., 2022). La implementación de medidas de seguridad en modelos de ML es un desafío constante, ya que tanto defensores como atacantes evolucionan continuamente en sus herramientas y técnicas (U. Verma et al., 2022). En el contexto de la creciente adopción de sistemas basados en ML, es esencial que los usuarios finales y los profesionales de la implementación reconozcan su responsabilidad en asegurar la protección de estos sistemas (Usama et al., 2020).

El Aprendizaje Máquina Adversaria (AML) ha surgido como una amenaza significativa para las aplicaciones basadas en ML, buscando socavar la adaptabilidad de los modelos de ML y convertirla en una desventaja (Ayub et al., 2020; Lagesse et al., 2016). Estas amenazas plantean desafíos importantes en la seguridad y confiabilidad de las aplicaciones de ML, destacando la necesidad de comprender y abordar el campo del AML para proteger adecuadamente nuestras infraestructuras y sistemas.

Los objetivos del AML incluyen degradar el rendimiento de un modelo de ML, manipular los datos de entrenamiento para violar políticas de seguridad, obtener información sobre inteligencia de amenazas y conocer las capacidades del adversario (Yeboah-Ofori et al., 2021). Estos objetivos establecen la base para el desarrollo de estrategias de ataque y defensa. Por ejemplo, los ataques de AML buscan alterar los datos y etiquetas para comprometer los modelos de ML, mientras que las medidas de mitigación buscan aumentar la robustez de los modelos para hacer frente a estos ataques sin comprometer su rendimiento (Usama et al., 2020; F. O. Catak et al., 2022).

Un ejemplo de los objetivos del AML son las perturbaciones adversarias que representan una amenaza significativa para las aplicaciones de Aprendizaje Máquina, introduciendo cambios mínimos en los datos de entrada para confundir los modelos y generar salidas incorrectas (Usama et al., 2019; Tang et al., 2019). Estos ataques buscan comprometer el rendimiento de los modelos al sobrepasar los límites de decisión, induciendo errores en las predicciones (Khalid et al., 2019).

La comprensión y mitigación de estos ataques son esenciales para el desarrollo de sistemas inteligentes seguros y confiables, lo que garantiza la integridad y la confianza de los usuarios en una amplia gama de aplicaciones (Guesmi et al., 2022).

Ataques de Aprendizaje Máquina Adversaria

Las estrategias de los adversarios para degradar el rendimiento de los modelos, incluyen la modificación de la entrada original del clasificador mediante pequeñas perturbaciones (Lin & Biggio, 2021).

Los adversarios calculan el gradiente de pérdida de una etiqueta con respecto a la entrada en el modelo para manipular la función de pérdida y lograr una clasificación incorrecta (Guihai & Sikdar, 2021). Un aspecto importante para el exito de un ataque de AML es cómo los humanos pueden tener dificultades para distinguir entre ejemplos normales y maliciosos, especialmente cuando se alteran sutilmente los datos de salida durante la fase de clasificación (Mehta et al., 2022).

El Aprendizaje Máquina Adversaria se ha convertido en una categoría de ataques duradera, lo que requiere una atención continua y estrategias de mitigación en constante desarrollo (Lin & Biggio, 2021; Hao & Tao, 2022). Conocer estos ataques es crucial para anticiparse a ellos y proteger los sistemas de Aprendizaje Máquina, lo que puede ahorrar costos y fortalecer la seguridad de los sistemas (Guihai & Sikdar, 2021).

-Dominios de Aprendizaje Máquina Adversaria

El principal desafío al aplicar técnicas de Aprendizaje Máquina Adversaria es la falta de investigación diversa, principalmente centrada en el reconocimiento de imágenes (Li et al., 2020; Hao & Tao, 2022; Marulli et al., 2021; Zizzo et al., 2019), dejando otros dominios como la ciberseguridad y el procesamiento de audio poco explorados. Esta disparidad se debe a que, a diferencia de los ataques en imágenes donde alterar un píxel tiene un impacto limitado, en otros contextos, como modificar un byte en un archivo de audio, un pequeño cambio puede hacer que el archivo se vuelva disfuncional o se corrompa, lo que representa un desafío importante en la creación de ataques adversarios en dominios donde la integridad y funcionalidad son críticas.

--Imagen

La investigación en Aprendizaje Máquina Adversaria ha estado predominantemente centrada en la visión por computadora, particularmente en el procesamiento de imágenes (Verma et al., 2022; Zhang & Sikdar, 2022). El AML se introdujo inicialmente como un método engañoso contra las redes neuronales en este campo (Zhang & Sikdar, 2022). Aunque se ha aplicado también en sistemas biométricos, marcas de agua y detección de anomalías en videos. Los modelos de Aprendizaje Profundo (Deep Learning) han demostrado ser particularmente efectivos en la clasificación de imágenes, como evidencia el uso de Google en su servicio de búsqueda de imágenes (Shinde & Shah, 2018).

Sin embargo, estos modelos son altamente vulnerables a ejemplos adversarios que pueden engañarlos para producir resultados incorrectos (Hao & Tao, 2022; Seo et al., 2022). Por ejemplo, en 2013, Szegedy demostró que las Redes Neuronales Profundas (DNN) pueden ser engañadas por imágenes manipuladas de manera imperceptible (Khalid et al., 2019). Los ataques adversarios en visión por computadora a menudo siguen una metodología de dos pasos, donde se introduce ruido aleatorio en la imagen objetivo para medir su imperceptibilidad, ajustándolo hasta que se logre un ejemplo adversario (Khalid et al., 2019).

Estos ataques pueden presentarse como modificaciones en los valores de píxeles de una imagen, engañando al modelo para interpretarla incorrectamente (Zhang & Sikdar, 2022; Edwards & Rawat, 2020). Otro ataque que pertenece al dominio de la imágen es el presentado por Quiring y Rieck (2018) quienes hacen un ataque contra esquemas de marcas de agua, utilizando un modelo sustituto basado en una Red Neuronal Profunda para engañar al detector de marcas de agua original. Este ataque, conocido como C&W, destaca por su efectividad y no requiere un conocimiento detallado del esquema de marcas de agua original.

*** En el caso de imágenes 3D Drenkow et al. (2023) proponen un ataque basado en perturbaciones adversarias llamadas parches, dirigido a secuencias de imágenes mediante optimización 3D a lo largo de trayectorias cinemáticas. Introducen "aadaptive active attacks" (AAA), considerando la postura del observador al que se dirige el ataque. Los resultados muestran que los ataques AAA pueden ser más efectivos que los estáticos, especialmente en situaciones de movimiento, subrayando la necesidad de desarrollar defensas contra estas nuevas amenazas dinámicas. ***

*** Yoon et al. (2023) proponen un marco para generar perturbaciones adversarias en tiempo real dirigidas a sistemas de detección/seguimiento de objetos en vehículos autónomos. Este enfoque utiliza redes generativas adversarias (GAN) para generar imágenes adversarias en línea, un agente de aprendizaje por refuerzo para desviar el vehículo según el objetivo del atacante, y un tomador de decisiones binario que determina cuándo utilizar ataques de imágenes. El marco ofrece una alternativa más eficiente y práctica a los métodos iterativos de caja blanca, siendo capaz de implementarse en tiempo real en bots del mundo real. ***

***Man et al. (2023) desarrollan un sofisticado ataque para alterar discretamente la percepción visual en sistemas de reconocimiento de objetos basados en cámaras. Proyectan patrones adversarios en las cámaras para engañar al sistema y provocar decisiones erróneas. Aunque no tiene un nombre específico, este ataque se basa en el principio de un ataque adversario, creando entradas perturbadas. Los resultados revelan cómo estos ataques pueden tener consecuencias significativas en aplicaciones críticas como vehículos autónomos y sistemas de vigilancia.***

--Procesamiento de Lenguaje Natural (NLP)

En el dominio del Procesamiento de Lenguaje Natural (NLP), los ataques adversarios presentan desafíos distintos a los observados en el procesamiento de imágenes. Marulli et al. (2021) destacan que los ataques diseñados para sistemas de imágenes no son efectivos cuando se aplican a representaciones vectoriales utilizadas en NLP. Edwards y Rawat (2020) señalan que los ejemplos adversarios en NLP pueden afectar el rendimiento del modelo al modificar semántica, ortografía o frases completas, aunque estas perturbaciones suelen ser perceptibles. Por ejemplo, Cresci et al. (2022) describen ataques contra sistemas de detección de noticias falsas, como "TextBugger", que manipulan el contenido de noticias para engañar clasificadores. Estos ataques influyen en el título, contenido o fuente de una noticia para revertir el resultado del clasificador. 

--Dominio inalámbrico 

En el ámbito inalámbrico, el aumento del uso de aplicaciones de Aprendizaje Máquina ha suscitado la necesidad de comprender los desafíos de seguridad asociados. Shi et al. (2019) señalan una falta de investigación sobre ataques de Aprendizaje Máquina Adversaria en la clasificación del tráfico de red, posiblemente debido a la complejidad de alterar los paquetes de datos sin cambiar su significado. Davaslioglu & Sagduyu (2019) presentan un ejemplo de ataque en el dominio inalámbrico, utilizando un ataque de envenenamiento tipo Backdoor para manipular el comportamiento de un modelo de Aprendizaje Máquina durante la fase de pruebas.

Por otro lado, Usama et al. (2019) y Usama, Qayyum, et al. (2019) investigan ataques adversarios en redes cognitivas autónomas y en sistemas de clasificación del tráfico TOR, respectivamente. Ambos estudios revelan que tanto los modelos de modulación basados en Aprendizaje Máquina como las técnicas de Deep Learning son vulnerables a ataques adversarios.

Además, Catak et al. (2021) destacan la importancia de la seguridad en las redes 6G y proponen el uso del "Fast Gradient Sign Method" (FGSM) para generar muestras adversarias y fortalecer la robustez de los modelos de Aprendizaje Máquina.

***Otro ataque de este dominio es presentado por Shi et al. (2023) donde describen un ataque inalámbrico basado en aprendizaje automático adversario dirigido a redes NextG. Este ataque manipula algoritmos de aprendizaje por refuerzo (RL) para bloquear selectivamente recursos, causando pérdidas en la red. Para contrarrestarlo, se proponen defensas como Q-Protect, que suspende actualizaciones de RL al detectar un ataque, y métodos como RandomOpt y RandomTop, que introducen aleatoriedad en las decisiones del algoritmo para confundir al adversario. Además, MisNACK manipula retroalimentación para evitar que el adversario construya su estrategia. Estas defensas protegen la asignación de recursos en redes NextG contra ataques adversarios basados en aprendizaje automático.***

--Ciberseguridad

El ámbito de la ciberseguridad enfrenta desafíos únicos en relación con los ataques de Aprendizaje Máquina Adversaria (AMA). Los sistemas de Aprendizaje Máquina (ML) en seguridad informática son más complejos y dinámicos, lo que los hace más vulnerables a ataques, aunque la investigación en este campo aún es limitada (Yeboah-Ofori et al., 2021; Shi et al., 2019).

Ataques contra sistemas DNS: Jin et al. (2019) abordan los ataques de envenenamiento a servidores DNS, destacando la importancia de técnicas de defensa como el entrenamiento adversario.

Ataques contra IDS: Anthi et al. (2021) y Ayub et al. (2020) investigan ataques de AML contra modelos de detección de intrusos (IDS), revelando una notable disminución en la precisión de los modelos después de ser atacados.

Ataques contra sistemas de detección de malware y SPAM: Chen et al. (2017) describen un ataque de evasión llamado "EnvAttack" que manipula características de malware para evadir la detección. Además, Yeboah-Ofori et al. (2021) mencionan ataques de AML en sistemas de detección de SPAM, resaltando la importancia de la defensa contra estos ataques.

***Ataques con sistemas detectores de Phising: Yang et al. (2023) investigan los desafíos y la incertidumbre en torno a los ataques adversarios en sistemas de aprendizaje automático (ML), especialmente en la detección de sitios web de phishing (PWD). Proponen un modelo de amenaza realista y evalúan exhaustivamente 12 ataques de evasión diferentes. Destacan la efectividad de los intentos de evasión más probables y el impacto de las perturbaciones en diferentes áreas de evasión. Subrayan que los ataques realistas pueden reducir la detección de phishing y señalan áreas para mejorar la seguridad de los sistemas de detección de phishing basados en ML.***

--Estados cuanticos y audio

Los modelos de clasificación basados en Aprendizaje Automático (ML) cuánticos ofrecen eficiencia en la clasificación de grandes volúmenes de datos, aunque enfrentan desafíos de seguridad similares a los modelos tradicionales. Edwards y Rawat (2020) advierten sobre el impacto negativo que pueden tener los ejemplos adversarios en los modelos de ML cuánticos, destacando la posibilidad de ataques como el Método del Signo de Gradiente Rápido (FGSM) y el riesgo de ataques de transferencia, aunque este último aún no se ha explorado ampliamente.

Es esencial realizar una revisión exhaustiva de las vulnerabilidades de los modelos de ML cuánticos antes de su implementación en producción, abordando las debilidades existentes para garantizar su robustez y seguridad.

En el dominio del procesamiento de audio, los ejemplos adversarios son menos explorados pero pueden tener un impacto significativo en el rendimiento de los modelos. Lin y Biggio (2021) presentan un caso de ataque adversario de evasión basado en audio dirigido al sistema de reconocimiento de voz a texto de Mozilla DeepSpeech, donde la inclusión de ruidos apenas perceptibles comprometió la precisión del modelo, resaltando la vulnerabilidad en este contexto específico.

-Perturbaciones

El concepto de perturbaciones o muestras adversarias es fundamental en el ámbito del Aprendizaje Máquina Adversaria (AML), donde los atacantes introducen datos maliciosos durante el entrenamiento y las pruebas de modelos de clasificación de Aprendizaje Máquina (ML) para engañar a estos modelos. Estas perturbaciones, casi imperceptibles para los humanos pero capaces de inducir clasificaciones incorrectas en los modelos, representan un desafío de seguridad significativo (Yeboah-Ofori et al., 2021; Ntalampiras, 2023; Usama et al., 2019; Guihai & Sikdar, 2021; McDaniel et al., 2016).

Christian Szegedy y sus colegas fueron los primeros en destacar la vulnerabilidad de las redes neuronales profundas frente a estas perturbaciones casi imperceptibles en las entradas, lo que puede llevar a predicciones erróneas en los modelos (Szegedy et al., 2013, citado por Liu et al., 2020).

Estas perturbaciones, también llamadas ejemplos adversarios, pueden ser diseñadas para maximizar el error de predicción del modelo (Usama et al., 2020; Khalid et al., 2019; Kurakin et al., 2016; Verma et al., 2022; Cresci et al., 2022; Seo et al., 2022). Pueden afectar diversos sistemas de ML, siendo los sistemas de Deep Learning especialmente vulnerables (Guihai & Sikdar, 2021; Kurakin et al., 2016; Tian et al., 2022; Liu et al., 2020; Marulli et al., 2021). La capacidad de transferibilidad de estas perturbaciones, es decir, su efectividad en diferentes modelos, es un aspecto importante a considerar (Lin & Biggio, 2021; Quiring & Rieck, 2018).

La generación de estas perturbaciones se ha convertido en un área de investigación activa, con métodos como el Fast Gradient Sign Method (FGSM) utilizado para crear perturbaciones óptimas (Davaslioglu & Sagduyu, 2019). *** Un ejemplo interesante es presentado por Nazari et al. (2023) quienes proponen "Cloak & Co-locate", una estrategia de ataque para la coubicación de máquinas virtuales del atacante con las de la víctima en la nube. Esta estrategia consta de ingeniería inversa y generación de muestras adversarias, estas últimas utilizadas para engañar al modelo de aprendizaje automático del Sistema de Protección de la Red (RPS). *** ***Otro ejemplo es mencionado en el estudio de Paladini et al. (2023), se investiga cómo debilitar los sistemas de detección de fraude (FDS) mediante la inserción deliberada de perturbaciones adversarias en el conjunto de entrenamiento. Esto tiene como objetivo alterar el modelo de detección de fraude para que tome decisiones incorrectas, lo que facilita la ejecución exitosa de actividades fraudulentas.***


Además de su uso en la generación de ataques, las perturbaciones adversarias también tienen aplicaciones en la ofuscación de modelos de ML, ayudando a proteger los modelos contra ataques maliciosos (G. Verma et al., 2018).

-Taxonomia 

Dentro del campo del Aprendizaje Máquina Adversaria (AML), varias propuestas de clasificación han surgido para categorizar las técnicas de ataques:
	-Taxonomía del NITS: Desarrollada por el Instituto Nacional de Estándares y Tecnología (NITS), divide los ataques en tres categorías principales: objetivos, técnicas y conocimiento (Tabassi et al., 2019).
	-Propuesta de Barreño: Organiza los ataques en tres ejes: influencia, violación de seguridad y especificidad (Barreno et al., 2006).
	-Categorización de Papernot: Considera la complejidad del ataque y el conocimiento del adversario sobre el modelo atacado (Papernot et al., 2016).
	-Taxonomía de Khalid et al.: Clasifica los ataques en tres categorías principales según el momento y método de ataque: entrenamiento, inferencia e implementación de hardware (Khalid et al., 2019).
	-Clasificación de Olney & Karam: Distingue entre ataques causales, dirigidos a subvertir el proceso de entrenamiento, y exploratorios, destinados a exponer información sobre la arquitectura subyacente del modelo (Olney & Karam, 2022).

Otras formas de clasificar los ataques de AML son:
	1.Ataques según el tipo de violación de seguridad:
		-Ataques de confiabilidad: Buscan maximizar el error de predicción del modelo, haciéndolo inútil, especialmente crítico en aplicaciones como seguridad y atención médica (Ma et al., 2020).
		-Ataques de integridad: Afectan la precisión del modelo al causar clasificaciones incorrectas y altas tasas de error, comunes en entornos adversarios durante la operación de sistemas de ML (Anthi et al., 2021).
		-Ataques de disponibilidad: Pretenden impedir la utilidad del sistema, induciendo errores en el modelo o denegando servicios, siendo conocidos también como ataques de envenenamiento (Mehta et al., 2022).
	2.Ataques según la especificidad del ataque:
		-Ataques dirigidos: Tienen un objetivo específico en mente, como cambiar la predicción del modelo a una salida fija y alternativa en un conjunto de datos específico (Mehta et al., 2022).
		-Ataques indiscriminados: Carecen de un objetivo específico y buscan alterar las clasificaciones de manera aleatoria (Liu et al., 2020).
		-Ataques combinados: Algunas metodologías, como FaDec propuesta por Khalid et al. (2020), pueden fusionar elementos de ataques dirigidos y no dirigidos para maximizar la eficiencia del ataque.
	4.Ataques en relación al impacto en el proceso de aprendizaje:
		-Ataques en Fase de Pruebas: Conocidos como ataques exploratorios, buscan manipular datos "no etiquetados" para evitar la detección durante la etapa de pruebas o inferencia (Ma et al., 2020).
		-Ataques en Fase de Entrenamiento: También llamados ataques causales, subvierten el proceso de entrenamiento al introducir ejemplos adversarios en el conjunto de datos de entrenamiento (Olney & Karam, 2022).
	5.Ataques en relación a la información (conocimiento) del adversario:
		-Ataques de caja blanca: El adversario tiene acceso completo a la información del modelo, facilitando la generación de perturbaciones adversarias (Ma et al., 2020).
		-Ataques de caja negra: El adversario carece de información sobre el modelo, construyendo un modelo sustituto para generar el ataque y aprovechando la transferibilidad (Verma et al., 2022).
		-Ataques de caja gris: El adversario opera con información parcial sobre el modelo, buscando influir en las fases de entrenamiento y prueba (Ayub et al., 2020).

Ataques de evasión

Los ataques de evasión, conocidos como Evassion Attacks, ocurren durante la fase de pruebas de un modelo de Aprendizaje Máquina. El objetivo es manipular los datos "NO ETIQUETADOS" para evitar la detección durante la etapa de prueba, sin alterar el proceso de entrenamiento (Ma et al., 2020; Olney & Karam, 2022; Venkatesan et al., 2021; Chen et al., 2017; Ebrahimabadi et al., 2021; Khalid et al., 2019; Zizzo et al., 2019; Catak et al., 2021; Anthi et al., 2021; Sun et al., 2022; Olney & Karam, 2022). Esto implica agregar ruido cuidadosamente a las instancias de los datos de prueba para crear perturbaciones adversarias (Ma et al., 2020; Lin & Biggio, 2021; Khalid et al., 2019), lo que permite evadir la capacidad del modelo existente (Hao & Tao, 2022). Estas perturbaciones pueden hacer que los modelos clasifiquen erróneamente las instancias como legítimas, lo que facilita ataques dirigidos o indiscriminados (F. O. Catak et al., 2022; Quiring & Rieck, 2018; Mehta et al., 2022). Ampliamente estudiados, estos ataques representan una amenaza importante, ya que pueden duplicar la probabilidad de error y no requieren modificar la estructura del modelo (Olney & Karam, 2022; Venkatesan et al., 2021). Este tipo de ataques se basan en gradientes y son más poderosos que los ataques sin gradientes, como el Fast Gradient Sign Method, el Iterative Gradient Sign o el ataque de C&W (Zizzo et al., 2019). Un ejemplo es el descrito por Verma et al. (2022), donde se alteran muestras de malware para evitar la detección. Catak et al. (2021) señalan que estos ataques se utilizan en casos de phishing, spam y análisis de malware. Además, W. Li et al. (2022) detallan un ataque que engaña a un clasificador SVM, técnica aplicable a SVM lineales según Frederickson et al. (2018). *** Otro ejemplo es presentado por Ntalampiras (2023) el cual investiga ataques adversarios en redes eléctricas inteligentes, enfocándose en los ataques de evasión que manipulan datos para inducir errores en modelos de aprendizaje automático, los resultados muestran una degradación en la detección de medidores comprometidos y la transferibilidad del enfoque a otros sistemas de detección de ataques. *** Estos ataques, también llamados ataques exploratorios, buscan exponer la arquitectura del modelo al proporcionar ejemplos contradictorios (Frederickson et al., 2018; Olney & Karam, 2022).		

Lista de ataques de evasión 

	Fast Gradient Sign Method (FGSM): El Fast Gradient Sign Method (FGSM), propuesto por Ian Goodfellow, busca aproximar linealmente la función de costo cerca de las muestras legítimas para generar rápidamente perturbaciones adversarias (Goodfellow et al., 2014, citado por Verma et al., 202; Goodfellow et al., 2014, citado por Liu et al., 2020). Este método se basa en el uso de gradientes para calcular perturbaciones que aumenten la función de pérdida y evadan la detección del modelo (Usama, Qayyum, et al., 2019; Usama et al., 2020; Usama et al., 2019; Kurakin et al., 2016; F. O. Catak et al., 2022; Catak et al., 2021; *** P, L. M. D., & Gunasekaran, 2023; *** *** Naqvi et al., 2023 ***). FGSM puede realizar ataques dirigidos y no dirigidos, controlando la norma L1, L2 o L∞ de la alteración (Ntalampiras, 2023). Aunque es simple y eficiente computacionalmente, se señalan debilidades, como su falta de robustez y su desempeño deficiente con gradientes pequeños (Zizzo et al., 2019; Usama et al., 2019; Mehta et al., 2022). Existen variantes del FGSM, como el Fast Gradient Value (FGV), que maximiza la pérdida en un solo paso (Guihai & Sikdar, 2021), y el Iterative Gradient Sign (IFGS), que agrega perturbaciones en múltiples iteraciones (Liu et al., 2020; Zizzo et al., 2019). Además, está el Projected Gradient Descent (PGD), una versión iterativa del FGSM que utiliza el algoritmo de retropropagación (Zhang & Sikdar, 2022). También se menciona el Basic Iterative Method (BIM), que aplica un método iterativo de optimización para generar perturbaciones (Usama et al., 2018, citado por Usama et al., 2020; F. O. Catak et al., 2022). Una variante del BIM es el Momentum Iterative Method (MIM), que introduce un impulso en el proceso (F. O. Catak et al., 2022). Finalmente, el Iterative Least-Likely Class Method, utilizado contra el modelo "Inception", demuestra su superioridad sobre las defensas adversarias (Kurakin et al., 2016).

	Ataque Jacobian-based Saliency Map Attack JSMA: El Jacobian-based Saliency Map Attack (JSMA), creado por Nicolas Papernot y colaboradores, es una estrategia iterativa para generar muestras adversarias, utilizando el jacobiano del modelo para determinar las perturbaciones necesarias para lograr una clasificación específica por parte del atacante (Papernot et al., p. 372-387 2016, citado por McDaniel et al., 2016; Papernot et al., p. 372-387 2016, citado por Usama, Qayyum, et al., 2019; Papernot et al., p. 372-387 2016, citado por Usama et al., 2020). Mehta et al. (2022) destacan que, en el contexto de un clasificador de imágenes, este enfoque manipula los píxeles de una imagen, saturándolos con sus valores máximos y mínimos para inducir una clasificación errónea. Además, Verma et al. (2022) presentan una aplicación en la que se utiliza el ataque JSMA en un sistema de clasificación de malware, demostrando los desafíos adicionales que presenta este tipo de clasificación debido a la naturaleza de los archivos PE. Otro caso ilustrativo es presentado por Ayub et al. (2020), donde el ataque JSMA se utiliza en sistemas basados en Aprendizaje Automático para la detección de intrusos (IDS), generando muestras de datos que confunden al modelo entrenado para clasificar datos maliciosos como benignos.

	DeepFool: El ataque conocido como DeepFool, propuesto por Moosavi-Dezfooli y colegas (Moosavi-Dezfooli et al., 2016, citado por Usama et al., 2019), tiene como objetivo evadir clasificadores de Aprendizaje Máquina mediante la generación iterativa de perturbaciones adversarias al linearizar el clasificador. Este método busca actualizar la muestra original hasta que esté ligeramente más allá del hiperplano de clasificación. Sin embargo, su capacidad de transferibilidad es limitada.

	L-BFGS: El ataque Broyden-Fletcher-Goldfarb-Shannon (L-BFGS), ideado por Szegedy (Szegedy et al., 2013, citado por Verma et al., 2022) y aplicado por Verma et al. (2018), se centra en la detección de perturbaciones adversarias en Redes Neuronales Profundas para ofuscar las muestras y agregar defensa al modelo.

	C&W: El ataque de C&W, basado en L-BFGS, es un enfoque iterativo para generar perturbaciones adversarias óptimas que optimizan la clasificación errónea mientras minimizan la distorsión introducida en las muestras (Xi, 2020, citado por Mehta et al., 2022). Este método ha demostrado ser efectivo en la elusión de clasificadores, incluso mostrando prometedores resultados en la reentrenamiento del modelo.

	EnvAttack: El ataque EnvAttack, presentado por Chen et al. (2017), manipula un subconjunto de características con un costo mínimo para evadir la detección, empleando un enfoque de envoltura para seleccionar características de manera efectiva.

	Mutual Information: El método Mutual Information, descrito por Usama et al. (2020), perturba el tráfico anómalo mientras garantiza el funcionamiento normal del sistema, utilizando el concepto de información mutua para identificar características distintivas y reducir la diferencia entre ellas.

	SIFA: El ataque Simple Iterative FDI Attack (SIFA) de Guihai y Sikdar (2021), dirigido a sistemas de detección de ataques FDI basados en Deep Learning, utiliza predicciones de demanda normales y perturbaciones calculadas mediante descenso de gradiente para evadir técnicas de detección y tener éxito en ataques de "Demanda Distribuida".

Ataques de Envenenamiento

Los ataques de envenenamiento, conocidos como "Model poisoning", tienen como objetivo principal introducir instancias con ruido y modificar las etiquetas de las muestras existentes durante la etapa de entrenamiento de un modelo (Ma et al., 2020; Verma et al., 2022; Lin & Biggio, 2021; Olney & Karam, 2022; Hao & Tao, 2022; Venkatesan et al., 2021; Marulli et al., 2021; Chiba et al., 2020; Tian et al., 2022; Baracaldo et al., 2018; Davaslioglu & Sagduyu, 2019). Estos ataques también se conocen como "ataques causales" (Sun et al., 2022; Shi et al., 2019; Frederickson et al., 2018; Olney & Karam, 2022), y pueden tener como finalidad la manipulación de un modelo para maximizar el error de clasificación y negar el servicio (Khalid et al., 2019). La inyección de muestras envenenadas cerca de datos de alta confianza afecta significativamente el proceso de entrenamiento, propagando eficazmente las etiquetas de las muestras envenenadas y resultando en una degradación del rendimiento del modelo resultante (Venkatesan et al., 2021).

Los ataques de envenenamiento son efectivos contra una variedad de algoritmos, incluidos los de Support Vector Machine (SVM) y Deep Learning (Frederickson et al., 2018), y han sido identificados como una preocupación principal para las empresas que implementan sistemas basados en Aprendizaje Máquina (Venkatesan et al., 2021). Estos ataques tienen el potencial de afectar diversas áreas más allá de las aplicaciones específicas existentes, como la visión por computadora, y pueden comprometer la integridad y disponibilidad de los modelos (Davaslioglu & Sagduyu, 2019). Los ataques de "Backdoor" son un componente crítico de los ataques de envenenamiento, capaces de reducir el rendimiento de un clasificador al inducir clasificaciones erróneas específicas o comportamientos incorrectos (Baracaldo et al., 2018). Los "troyanos neuronales" también son mencionados por Olney & Karam (2022) como capaces de alterar drásticamente el comportamiento y el rendimiento de una Red Neuronal Convolucional (CNN) mediante la modificación de datos de entrenamiento durante el proceso de capacitación.

Estos ataques también exhiben la propiedad de transferibilidad, lo que significa que las muestras que afectan a un modelo sustituto pueden transferirse y perjudicar al modelo objetivo (Chiba et al., 2020; Hao & Tao, 2022).


Lista de ataques de envenenamiento
	Ataque de Boling Frog: El Boling Frog es un tipo de ataque sofisticado que busca insertar gradualmente instancias envenenadas en un sistema. Su objetivo principal es eludir la detección y, secundariamente, influir en el rendimiento del clasificador subyacente. Se caracteriza por su enfoque sutil y estratégico en la introducción de instancias maliciosas (Lagesse et al., 2016; Ma et al., 2020).

	Outler detection evasion term: Frederickson et al. (2018) emplean una estrategia propuesta por Xiao y sus colegas, llamada "Poissoning Embedded Feature Selection", para crear un único punto de ataque dirigido a algoritmos de selección de características integradas, como Lasso. Esta estrategia se basa en el algoritmo de ascenso de gradiente con un tamaño de paso constante para perturbar la dirección de optimización y mantener efectivo el punto de ataque. La innovación de Frederickson et al. radica en la incorporación de un "Término de Penalización" en la función objetivo del atacante, que permite generar puntos de ataque que maximizan el impacto en la función objetivo del aprendiz y minimizan la probabilidad de ser detectados por el defensor. Los experimentos realizados demuestran que esta modificación permite crear instancias de ataque capaces de evadir la detección de valores atípicos en clasificadores entrenados en conjuntos de datos del mundo real.

	ATTack on Federated Learning (AT2FL): Sun et al. (2022) proponen el "ATTack on Federated Learning" (AT2FL) como respuesta al riesgo de ataques de envenenamiento de datos en nodos de Federated Learning. Este enfoque de optimización consciente del sistema eficientemente deriva gradientes implícitos de los datos envenenados para lograr estrategias de ataque óptimas en este contexto. AT2FL formula el envenenamiento de datos como un problema de optimización de dos niveles adaptable a diversas configuraciones de nodos. Los experimentos con un Modelo de Federated Machine Learning entrenado con datos reales confirman el impacto significativo del ataque en todos los conjuntos de datos, validando la vulnerabilidad del modelo.

	Ataque Adversarial Label Flips (LFA): Olney & Karam (2022) describen el "Ataque Adversarial Label Flips" (LFA), concebido por Huang Xiao y Claudia Eckert en 2012, como una táctica de envenenamiento que ha sido objeto de estudio detallado. Este ataque busca minimizar la cantidad de muestras contaminadas mientras maximiza la tasa de errores de clasificación en el modelo objetivo. No necesita un disparador para activarse, lo que lo convierte en un riesgo persistente. Los autores utilizan una versión ambigua del LFA para simular aumentos en la tasa de error y cambios en los parámetros de una red durante la fase de reentrenamiento, destacando la importancia de la detección temprana y la protección contra estas amenazas en el aprendizaje automático. *** Sánchez et al. (2023) utilizan Ataque Adversarial Label Flips (LFA) para hacer resistente a ataques adversarios un modelo de aprendizaje federado (FL) y proteger los datos de los usuarios. Comparan el rendimiento de identificación del modelo de dispositivo entre un modelo centralizado de aprendizaje profundo y uno federado, y evalúan el impacto de este ataque durante el entrenamiento del modelo federado. ***

	False Data Injection Attack: Tian et al. (2022) proponen un método que aprovecha los ataques de inyección de datos falsos para crear perturbaciones adversarias sutiles y de pequeño tamaño, evitando la detección al agregar perturbaciones a variables de estado no atacadas. Sus resultados revelan la efectividad de estos ataques en eludir métodos de detección de datos incorrectos y de ataques neuronales, mostrando la vulnerabilidad de estos métodos en esta situación.

	Ataque Binary-Search (Búsqueda Binaria): El "Ataque Binary-Search" (Búsqueda Binaria), según detallado por Ma et al. (2020), se basa en considerar la instancia objetivo como un valor atípico con respecto a los datos de entrenamiento de la clase opuesta. Su objetivo principal es generar instancias envenenadas que conecten el objetivo con la clase deseada para atenuar la influencia de la clase objetivo. En cada iteración, el ataque utiliza un punto medio entre la muestra objetivo y su vecino más cercano en la clase opuesta como candidato de envenenamiento. Si pertenece a la clase deseada, se agrega al conjunto de datos de entrenamiento y se ajusta el límite de clasificación gradualmente hacia el objetivo hasta invertir su etiqueta. Si no cumple con esta premisa, se reinicia el proceso.

	Ataque StingRay (Ataque de mantarraya): El "Ataque StingRay" (Ataque de mantarraya) según Ma et al. (2020), introduce nuevas copias de instancias al perturbar características menos críticas. Similar al ataque Binary-Search, selecciona una instancia base cercana al objetivo en la clase deseada, crea una copia de esta instancia y la perturba. La instancia envenenada más cercana al objetivo se introduce en los datos de entrenamiento.

	Adversarial Attack on RF & GBoost Classifiers: Yeboah-Ofori et al. (2021) proponen un ataque adversario dirigido a clasificadores Random Forest y Gradient Boosting. El ataque se basa en desplazar puntos dentro de las muestras más allá del hiperplano más cercano para inducir perturbaciones y posibles clasificaciones incorrectas. Los experimentos se enfocan en predecir un ataque de "ransomware", alterando la detección de intrusiones en la red mediante la inserción maliciosa de una entrada en las funciones, comprometiendo así el rendimiento del modelo.

	Gradient ascent y High confidence data poisoning: Venkatesan et al. (2021) llevaron a cabo experimentos sobre la manipulación de modelos de Aprendizaje Máquina utilizados en la detección de intrusiones en redes (NIDS). Exploraron dos tipos de ataques de envenenamiento. El primero, llamado "Gradient ascent", ajusta un punto en la dirección de una clase objetivo mediante el cálculo del gradiente de la función de pérdida. Aunque provocó errores significativos en los conjuntos de validación y prueba, sus gradientes convergieron rápidamente. El segundo ataque, "High confidence data poisoning", expande el conjunto de datos introduciendo perturbaciones cerca de muestras de alta confianza, añadiendo etiquetas que se asemejan a estas, lo que afecta significativamente el proceso de entrenamiento.

	Ataques Backdoor y Troyanos neuronales: Los ataques de puerta trasera (Backdoors) son una táctica fundamental en el Aprendizaje Máquina Adversario, donde se insertan patrones especiales, llamados "disparadores", durante el entrenamiento del modelo para activarse más tarde y desencadenar un comportamiento específico (Lin & Biggio, 2021; Marulli et al., 2021; Chiba et al., 2020; Olney & Karam, 2022). Estos ataques pueden manipular la canalización de entrenamiento al introducir muestras contaminadas (Davaslioglu & Sagduyu, 2019; Venkatesan et al., 2021), lo que plantea preocupaciones de seguridad, especialmente en datos de código abierto (Lin & Biggio, 2021). Los "troyanos neuronales" son una variante de estos ataques, difíciles de detectar ya que solo se activan con entradas específicas conocidas por el atacante (Olney & Karam, 2022). Se introducen durante el entrenamiento y pueden afectar la clasificación en la fase de pruebas (Davaslioglu & Sagduyu, 2019). Un ejemplo específico incluye la inserción de palabras envenenadas en un modelo de Reconocimiento de Entidades Nombradas (NER) (Marulli et al., 2021). Estos ataques, dirigidos a datos públicos en línea, representan una amenaza emergente (Davaslioglu & Sagduyu, 2019).

	***BO-FLPA y BO-SLPA: Aristodemou et al. (2023) emplean la optimización bayesiana para envenenar modelos de aprendizaje federado (FL) y de aprendizaje dividido (SL), respectivamente. En BO-FLPA, se busca aumentar la susceptibilidad del modelo FL manipulando los pesos de las capas ocultas para maximizar la incertidumbre de clasificación en características clave. Por otro lado, en BO-SLPA, se modifica la distribución de pesos del modelo SL para debilitar la confianza del modelo global mediante la manipulación de características importantes del lado del servidor. Ambos ataques apuntan a disminuir la precisión de la predicción y aumentar la incertidumbre en las predicciones, subrayando la necesidad de desarrollar defensas robustas contra tales ataques en entornos de aprendizaje distribuido como FL y SL. ***

Ataques adversarios de modelado/transferencia

Las perturbaciones adversarias exhiben transferibilidad, lo que significa que pueden afectar a modelos similares incluso con diferentes arquitecturas y datos de entrenamiento (Lin & Biggio, 2021; Usama et al., 2020; Verma et al., 2022; Olney & Karam, 2022). Papernot et al. (2016, citado por Hao & Tao, 2022) introdujeron la noción de transferibilidad intratecnológica y entre diferentes tipos de modelos. Esta característica es explotada en ataques de caja gris y negra (Verma et al., 2022).

Estos ataques, también llamados ataques de transferencia, extracción o robo de modelos, aprovechan esta propiedad (Edwards & Rawat, 2020; Quiring & Rieck, 2018; Lin & Biggio, 2021). Por ejemplo, Zhang y Sikdar (2022) presentan ETAA, un método que mejora la capacidad de transferencia de ataques adversarios entre distintos modelos de Deep Learning. ETAA opera en dos etapas, simulando un ataque de caja blanca en un conjunto de modelos y luego eligiendo un modelo para atacar como caja negra, lo que mejora la transferibilidad de las perturbaciones.

Quiring y Rieck (2018) abordan el aprendizaje de un modelo sustituto para eliminar marcas de agua en señales. Después de encontrar un modelo sustituto, aplican un descenso de gradiente para suprimir las marcas de agua en la señal marcada, lo que permite eludir la detección por el clasificador original. Este proceso se realiza ajustando la formulación de C&W y aplicando un descenso de gradiente hasta que el modelo sustituto predice la ausencia de la marca de agua con alta confianza.

Ataques adversarios como defensa

Las perturbaciones adversarias no solo representan una amenaza para los modelos de Aprendizaje Automático, sino que también pueden ser utilizadas para reforzarlos (Cresci et al., 2022). Los ataques de Aprendizaje Máquina Adversaria incluso se han identificado como métodos defensivos potenciales (Ebrahimabadi et al., 2021). Por ejemplo, Yilmaz y Siraj (2021) presentan AMLODA, un modelo que busca ocultar patrones de consumo de electricidad mediante perturbaciones mínimas e imperceptibles en los datos.

Otro ejemplo es propuesto por Ebrahimabadi et al. (2021), donde se sugiere envenenar los Desafíos-Respuestas transmitidos entre nodos de dispositivos IoT y un servidor para prevenir ataques de repetición o suplantación de identidad.

G. Verma et al. (2018) proponen un método para que un defensor logre sus objetivos de ofuscación utilizando el ataque L-BFGS, generando perturbaciones que hacen que una muestra sea prácticamente indistinguible de la original en términos de tamaño del paquete.

Técnicas de mitigación de Aprendizaje Máquina Adversaria

La protección y fortalecimiento de los modelos de Aprendizaje Máquina son cruciales para enfrentar los ataques de Aprendizaje Máquina Adversaria, con un creciente interés en identificar problemas de diseño y desafíos de investigación para defenderse contra estos ataques (Ma et al., 2020). Las estrategias de defensa incluyen enmascarar gradientes, optimización robusta y detección de adversarios (Verma et al., 2022).

Jin et al. (2019) señalan soluciones potenciales para detectar ataques de envenenamiento, como la inclusión de muestras etiquetadas como válidas y técnicas de extracción de características. Estos enfoques se clasifican en defensas de detección y defensas de prevención (Usama et al., 2020), siendo las Defensas Reactivas y Proactivas los principales enfoques para mitigar el Aprendizaje Máquina Adversaria.

Además, estrategias como evitar la exposición de puntos de confianza y utilizar herramientas como Code2Graph contribuyen a fortalecer la seguridad de los sistemas basados en Aprendizaje Máquina (Mehta et al., 2022).

Defensas reactivas

Una defensa reactiva en el contexto del Aprendizaje Máquina Adversaria implica reconfigurar o reentrenar el modelo después de un ataque para salvaguardar información crítica, identificar y mitigar los efectos del ataque (Usama et al., 2020). Estrategias como la detección adversaria, la reconstrucción de la entrada y la validación de la red se utilizan para contrarrestar las perturbaciones adversarias (Hao y Tao, 2022). Un ejemplo es "Ocultar el vector de probabilidad", eficaz contra ataques de caja negra (Khalid et al., 2020).

El análisis de amenazas, resaltado por Lin y Biggio (2021), proporciona una visión general de las amenazas en servicios basados en Aprendizaje Máquina, facilitando la identificación de puntos críticos. Iniciativas como el marco de matriz de amenazas de Microsoft y el MITRE ayudan en el análisis de seguridad para tomar decisiones informadas y aplicar medidas adecuadas.

La ofuscación de datos, explorada por G. Verma et al. (2018), busca ocultar patrones en la clasificación de firmas estadísticas de red. Esta técnica altera la distribución de los tamaños de paquetes para confundir al atacante, utilizando el concepto de transferibilidad para efectos de ofuscación sobre el modelo original. Los resultados muestran estrategias efectivas para camuflar el tráfico de red de manera sorprendentemente simple.

Defensas proactivas

Las estrategias proactivas en el Aprendizaje Máquina Adversaria buscan evitar ataques y preparar modelos para resistir perturbaciones maliciosas, construyendo modelos más sólidos y resistentes (Usama et al., 2020; Lin & Biggio, 2021). Ejemplos incluyen destilación y entrenamiento adversario, así como agregar muestras válidas al conjunto de datos original y emplear métodos de extracción de características contra envenenamiento (Ma et al., 2020). Estas estrategias se centran en integraciones, entregas y capacitaciones continuas, y son fundamentales y ampliamente exploradas en la literatura.

Técnicas de seguridad tradicionales contra AML

Existen técnicas tradicionales que pueden fortalecer la robustez y resistencia a ataques en sistemas de Aprendizaje Máquina, aunque no se diseñaron específicamente para contrarrestar el Aprendizaje Máquina Adversaria (AML). Estas técnicas proporcionan una capa adicional de defensa contra ataques similares. Sin embargo, es importante tener en cuenta que estas técnicas pueden ser insuficientes en la práctica (Verma et al., 2022).

Una estrategia dentro de esta categoría es el análisis de los datos de entrenamiento, que identifica datos potencialmente maliciosos al examinar su origen y metadatos asociados (Baracaldo et al., 2018). Otra estrategia es la refinación del proceso de entrenamiento, que suaviza los límites de decisión del modelo o calcula la probabilidad de que una entrada sea una muestra adversaria según sus características (McDaniel et al., 2016).

Además, G. Verma et al. (2018) mencionan el cifrado como una herramienta para eliminar datos identificables en un clasificador de tráfico de red mientras se conservan características reconocibles, como el tamaño del paquete y los intervalos de llegada.

Lista de defensas proactivas

	Entrenamiento adversario: El enfoque de Adversarial Training, también conocido como entrenamiento adversario, fue propuesto inicialmente por Ian J. Goodfellow y su equipo en 2014 y posteriormente retomado por Ruitong Huang en 2015. Estudios han demostrado que reentrenar una red neuronal en un conjunto de datos que incluye tanto muestras originales como adversarias mejora significativamente su eficiencia y le otorga robustez ante perturbaciones adversarias. Este método desafía a los defensores a actuar como atacantes, generando ejemplos adversarios contra su propio modelo y luego entrenándolo para que no sea vulnerable a estos ataques. El entrenamiento adversario ha demostrado ser efectivo en diversos contextos. Anthi et al. (2021) lo aplicaron utilizando una validación cruzada de 10 veces, reduciendo el impacto de los modelos al reentrenarlos con datos de entrenamiento recién generados, lo que resultó en un aumento en el rendimiento de clasificación para varias combinaciones de ataques. Liu et al. (2020) lo emplearon para mejorar la robustez de los detectores de puntos de acceso basados en CNN, reduciendo el éxito de las perturbaciones adversarias sin afectar la precisión del modelo. *** P, L. M. D., & Gunasekaran (2023) utilizando este método en contra del ataque FGSM  *** .Verma et al. (2022) observaron un aumento significativo en la precisión de los clasificadores de detección de malware después de aplicar el entrenamiento adversario. Catak et al. (2021) utilizaron el entrenamiento adversario iterativo para hacer que los modelos de predicción de HAZ de ondas milimétricas fueran inmunes a ataques de ruido. Tang et al. (2019) propusieron un método de entrenamiento adversario en cinco pasos para fortalecer los modelos de pronóstico de carga basados en ANN. Además, esta técnica se ha utilizado para ampliar conjuntos de datos de entrenamiento y mejorar la detección de amenazas de malware. *** Tian et al. (2021) utilizan el entrenamiento adversario en contra de un ataque basado en redes neuronales.***

	Destilación Defensiva: La destilación defensiva, desarrollada por Papernot y Hinton en 2015, se presenta como una defensa contra perturbaciones adversarias en redes neuronales profundas. Oculta los gradientes entre la salida previa a la función softmax y esta misma función para brindar robustez contra ataques basados en gradientes como FGSM y BIM. Además de mejorar la resistencia a los ataques, reduce el tamaño de las redes neuronales, los costos computacionales y previene el sobreajuste del modelo. Sin embargo, tiene limitaciones. Verma et al. (2022) emplean esta técnica entrenando dos modelos, utilizando el conocimiento destilado del primero para entrenar el segundo, y ajustando la temperatura de destilación en la salida softmax. Otra implementación de este método, como la de F. O. Catak et al. (2022) en un modelo de IRS para redes 5G, emplea dos modelos, uno como maestro y otro como alumno, para enfrentar ataques como FGSM, BIM y MIM, mostrando alta efectividad contra BIM y MIM.

	Filtrado de datos: El filtrado de datos implica separar datos de entrenamiento malignos de muestras completamente malignas. Baracaldo et al. (2018) proponen un filtro basado en el "Marco de procedencia" para detectar datos peligrosos antes de la implementación del modelo. Este método segmenta los datos no confiables en grupos basados en metadatos de procedencia y evalúa el rendimiento del clasificador entrenado con y sin cada grupo. Amplifica el impacto de los datos venenosos para una detección más efectiva y escalable. Los experimentos muestran su eficacia en situaciones donde una fracción de los datos de entrenamiento está corrupta. Khalid et al. (2019) mencionan otros métodos, como el preprocesamiento con filtros de bajo peso y filtros de suavizado, para neutralizar los efectos de ataques adversarios en la clasificación de imágenes.

	Métodos de detección: Los métodos de detección buscan identificar perturbaciones adversarias en los datos de entrenamiento. Un enfoque optimiza la función del kernel en una SVM para hacer las perturbaciones más visibles, mientras que otro busca detectar muestras adversarias mediante clasificación directa o evaluación de la incertidumbre de la red neuronal (W. Li et al., 2022). Sin embargo, estos métodos pueden ser vulnerables a ataques adversarios. La "Detección de anomalías" utiliza modelos predictivos para detectar datos maliciosos retrospectivamente. Además, la "Detección de valores atípicos" utiliza métodos estadísticos para identificar valores anómalos en la activación de la última capa oculta de una red neuronal. Aunque estos métodos son efectivos, pueden tener limitaciones cuando el número de muestras envenenadas es pequeño (Anthi et al., 2021).


	Comprensión de características, Reducción de ruido y Despolarización: El método de "comprensión de características" busca reducir el espacio de búsqueda para adversarios fusionando muestras correspondientes a diferentes vectores de características en una sola muestra. Edwards y Rawat (2020) exploran estrategias defensivas en el Aprendizaje Máquina Cuántico, como la "reducción de ruido" y la "despolarización", que ayudan a proteger los modelos contra ataques adversarios. Estas defensas están ganando popularidad para mejorar la seguridad de los modelos de Aprendizaje Máquina.

	*** ROSA: Zhao et al. (2023) proponen ROSA (Robust Online Stability Assessment), una estrategia innovadora para fortalecer modelos de evaluación de la estabilidad eléctrica basados en ML contra perturbaciones adversas. Consta de dos módulos: uno supervisado para la estabilidad primaria y otro auto-supervisado para purificar muestras adversas. ROSA mejora la precisión incluso frente a perturbaciones, ofreciendo una solución competitiva para aplicaciones del mundo real. Sugieren explorar perturbaciones más complejas en futuras investigaciones. ***

	*** BDB: Aboutalebi et al. (2023) proponen DBD (Dual Model Divergence) Limited, un mecanismo defensivo para mejorar la precisión de las redes neuronales profundas en datos limpios y mantener la robustez contra ataques adversarios. Utiliza una medida de varianza, la Varianza DBD, para decidir la predicción final del modelo. Este enfoque no requiere entrenamiento adicional y puede integrarse con otros mecanismos de defensa. La principal limitación es el costo computacional de usar dos modelos simultáneamente. ***

	*** Hierarchical clustering: McCarthy et al. (2023) utilizan una medida de mitigación basada en "Hierarchical clustering", donde se busca reducir la superficie del ataque que una perturbación adversaria puede explotar dentro de las limitaciones del espacio de parámetros de un ataque. Esta medida de defensa se aplica en el contexto de clasificadores de tráfico de red. ***


Métricas de evaluación

Métricas de evaluación enfocadas en ataques de Aprendizaje Máquina Adversaria
	-Tasa de Éxito de Perturbación: Utilizada para medir el porcentaje de muestras originales perturbadas con éxito, como en el estudio de Liu et al. (2020) en un clasificador CNN.
	-Tasa de Éxito ASR: Representa el número de veces que un ataque tiene éxito en la predicción de la clase objetivo, empleada por Zhang y Sikdar (2022) y Ntalampiras (2023).
	-Confianza de Clasificación y Perturbación: Métricas utilizadas para evaluar el rendimiento de un clasificador frente a ataques, como C&W, FGSM, PGD y JSMA, mencionadas por Ntalampiras (2023).
	-Recall (Sensibilidad): Usado para cuantificar los resultados de ataques como SIFA, FGV, FGSM y DeepFool contra una CNN por Guihai y Sikdar (2021).
	-Precisión: Una medida de desempeño de sistemas de Aprendizaje Máquina, aplicada para evaluar el éxito de perturbaciones adversarias, como el ataque "Adversarial Sample Crafting Algorithm" (Usama, Qayyum, et al., 2019).
	-Especificidad: Utilizada para medir la efectividad de un ataque de envenenamiento en una DNN, mencionada por Marulli et al. (2021).
	-Error Cuadrático Medio MSE: Empleado para evaluar la efectividad de ataques como BIM, FGSM, PGD y MIM, según F. O. Catak et al. (2022).
	-Norma de Perturbación D, SSIM y CC: Métricas utilizadas para evaluar la imperceptibilidad de un ataque adversario en clasificadores de imágenes, en la metodología FaDec (Khalid et al., 2020).
	-Puntuación F1: Calculada como la media armónica de precisión y recuperación, mencionada por Yilmaz y Siraj (2021).
	-Exactitud: Usada para medir la efectividad del modelo AMLODA, según Yilmaz y Siraj (2021).
	-Probabilidad de Detección Errónea y Probabilidad de Falsa Alarma: Utilizadas para evaluar un modelo de Deep Learning frente a perturbaciones adversarias generadas por GAN, por Shi et al. (2019).
	-Distancia de Límite de Decisión y Costo Mínimo para un Ataque Exitoso: Empleadas para evaluar la efectividad de ataques como Binary Search y StingRay, según Ma et al. (2020).
	-Estabilidad de Inferencia y Tasa de Clasificación Errónea: Métricas utilizadas por Usama et al. (2020) para evaluar la efectividad del ataque Mutual Information.


Metricas de evaluación enfocadas en defensas de Aprendizaje Máquina Adversaria
	-Tasa de Falsos Positivos (FPR) y Tasa de Falsos Negativos (FNR): Métricas para evaluar el modelo AMLODA, donde FPR mide la proporción de muestras negativas incorrectamente clasificadas como positivas, y FNR indica la proporción de muestras positivas incorrectamente clasificadas como negativas (Yilmaz & Siraj, 2021).
	-Resiliencia del Modelo: Define la capacidad del modelo para resistir perturbaciones en las entradas. Cuanto mayor sea la perturbación necesaria para cambiar la clasificación de una muestra de su clase legítima a su clase adversaria, más robusto será el modelo (McDaniel et al., 2016).
	-Robustez: Métrica que compara la robustez del clasificador bajo diferentes parámetros de Kernel, utilizada para evaluar la calidad del ataque de evasión "Example Generation Against SVM" y la defensa "Kernel Parameter Optimization" (W. Li et al., 2022).
	-Root Mean Square Error (RMSE): Utilizado para evaluar la calidad del entrenamiento adversario en Redes Neuronales Artificiales (ANN), calculado como la raíz cuadrada del promedio de los errores cuadrados entre las predicciones del modelo y los valores reales (Tang et al., 2019).

