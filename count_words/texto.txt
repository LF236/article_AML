AML es un campo que ha surgido en respuesta al uso de tecnicas de Aprendizaje Máquina en la industria. Este se introdujo en el año 2013 para constrarrestar las vulnerabilidades de las redes neuronales en vision artififial, sin embargo sus raices se remontan a clasificadores de spam en los 2000 (Frederickson et al. 2018) y a teorías de aprendizaje computacional en presencia de ruido malicioso en 1993 (Frederickson et al., 2018). AML se define de diversas maneras, desde llevar a cabo ataques contra sistemas basados en ML hasta examinar sus vulnerabilidades y proponer estrategias de defensa (Anthi et al., 2021; Lin & Biggio, 2021; Mehta et al., 2022; Ebrahimabadi et al., 2021; Mumcu et al., 2022). La literatura sobre AML aborda una variedad de aspectos, desde técnicas de ataque hasta medidas de mitigación, centradas en dominios específicos como procesamiento del lenguaje natural, redes vehiculares o ciberseguridad (Alsmadi et al., 2022; Qayyum et al., 2020; Martins et al., 2020; Maiorca et al., 2020; Olney & Karam, 2022). 

Consideraciones previas de Aprendizaje Máquina aplicadas a Aprendizaje Máquina Adversaria 

-Problemas del Aprendizaje Máquina

El Aprendizaje Máquina (ML) es una tecnología ampliamente utilizada en diversos campos, pero su susceptibilidad a fallos puede ser explotada por actores malintencionados, comprometiendo la integridad, confidencialidad o disponibilidad de los modelos o sistemas de ML (Mehta et al., 2022; Wilhjelm & Younis, 2020). 

No hay un algoritmo universalmente superior en ML y los adversarios están cada vez más interesados en atacar sistemas basados en ML, motivados por el robo de información, sabotaje u objetivos económicos (Anthi et al., 2021; Mehta et al., 2022). La implementación de medidas de seguridad en modelos de ML es un desafío constante, ya que tanto defensores como atacantes evolucionan continuamente en sus herramientas y técnicas (U. Verma et al., 2022). En el contexto de la creciente adopción de sistemas basados en ML, es esencial que los usuarios finales y los profesionales de la implementación reconozcan su responsabilidad en asegurar la protección de estos sistemas (Usama et al., 2020).

El Aprendizaje Máquina Adversaria (AML) ha surgido como una amenaza significativa para las aplicaciones basadas en ML, buscando socavar la adaptabilidad de los modelos de ML y convertirla en una desventaja (Ayub et al., 2020; Lagesse et al., 2016). Estas amenazas plantean desafíos importantes en la seguridad y confiabilidad de las aplicaciones de ML, destacando la necesidad de comprender y abordar el campo del AML para proteger adecuadamente nuestras infraestructuras y sistemas.

Los objetivos del AML incluyen degradar el rendimiento de un modelo de ML, manipular los datos de entrenamiento para violar políticas de seguridad, obtener información sobre inteligencia de amenazas y conocer las capacidades del adversario (Yeboah-Ofori et al., 2021). Estos objetivos establecen la base para el desarrollo de estrategias de ataque y defensa. Por ejemplo, los ataques de AML buscan alterar los datos y etiquetas para comprometer los modelos de ML, mientras que las medidas de mitigación buscan aumentar la robustez de los modelos para hacer frente a estos ataques sin comprometer su rendimiento (Usama et al., 2020; F. O. Catak et al., 2022).

Un ejemplo de los objetivos del AML son las perturbaciones adversarias que representan una amenaza significativa para las aplicaciones de Aprendizaje Máquina, introduciendo cambios mínimos en los datos de entrada para confundir los modelos y generar salidas incorrectas (Usama et al., 2019; Tang et al., 2019). Estos ataques buscan comprometer el rendimiento de los modelos al sobrepasar los límites de decisión, induciendo errores en las predicciones (Khalid et al., 2019).

La comprensión y mitigación de estos ataques son esenciales para el desarrollo de sistemas inteligentes seguros y confiables, lo que garantiza la integridad y la confianza de los usuarios en una amplia gama de aplicaciones (Guesmi et al., 2022).

Ataques de Aprendizaje Máquina Adversaria

Las estrategias de los adversarios para degradar el rendimiento de los modelos, incluyen la modificación de la entrada original del clasificador mediante pequeñas perturbaciones (Lin & Biggio, 2021).

Los adversarios calculan el gradiente de pérdida de una etiqueta con respecto a la entrada en el modelo para manipular la función de pérdida y lograr una clasificación incorrecta (Guihai & Sikdar, 2021). Un aspecto importante para el exito de un ataque de AML es cómo los humanos pueden tener dificultades para distinguir entre ejemplos normales y maliciosos, especialmente cuando se alteran sutilmente los datos de salida durante la fase de clasificación (Mehta et al., 2022).

El Aprendizaje Máquina Adversaria se ha convertido en una categoría de ataques duradera, lo que requiere una atención continua y estrategias de mitigación en constante desarrollo (Lin & Biggio, 2021; Hao & Tao, 2022). Conocer estos ataques es crucial para anticiparse a ellos y proteger los sistemas de Aprendizaje Máquina, lo que puede ahorrar costos y fortalecer la seguridad de los sistemas (Guihai & Sikdar, 2021).

-Dominios de Aprendizaje Máquina Adversaria

El principal desafío al aplicar técnicas de Aprendizaje Máquina Adversaria es la falta de investigación diversa, principalmente centrada en el reconocimiento de imágenes (Li et al., 2020; Hao & Tao, 2022; Marulli et al., 2021; Zizzo et al., 2019), dejando otros dominios como la ciberseguridad y el procesamiento de audio poco explorados. Esta disparidad se debe a que, a diferencia de los ataques en imágenes donde alterar un píxel tiene un impacto limitado, en otros contextos, como modificar un byte en un archivo de audio, un pequeño cambio puede hacer que el archivo se vuelva disfuncional o se corrompa, lo que representa un desafío importante en la creación de ataques adversarios en dominios donde la integridad y funcionalidad son críticas.

--Imagen

La investigación en Aprendizaje Máquina Adversaria ha estado predominantemente centrada en la visión por computadora, particularmente en el procesamiento de imágenes (Verma et al., 2022; Zhang & Sikdar, 2022). El AML se introdujo inicialmente como un método engañoso contra las redes neuronales en este campo (Zhang & Sikdar, 2022). Aunque se ha aplicado también en sistemas biométricos, marcas de agua y detección de anomalías en videos. Los modelos de Aprendizaje Profundo (Deep Learning) han demostrado ser particularmente efectivos en la clasificación de imágenes, como evidencia el uso de Google en su servicio de búsqueda de imágenes (Shinde & Shah, 2018).

Sin embargo, estos modelos son altamente vulnerables a ejemplos adversarios que pueden engañarlos para producir resultados incorrectos (Hao & Tao, 2022; Seo et al., 2022). Por ejemplo, en 2013, Szegedy demostró que las Redes Neuronales Profundas (DNN) pueden ser engañadas por imágenes manipuladas de manera imperceptible (Khalid et al., 2019). Los ataques adversarios en visión por computadora a menudo siguen una metodología de dos pasos, donde se introduce ruido aleatorio en la imagen objetivo para medir su imperceptibilidad, ajustándolo hasta que se logre un ejemplo adversario (Khalid et al., 2019).

Estos ataques pueden presentarse como modificaciones en los valores de píxeles de una imagen, engañando al modelo para interpretarla incorrectamente (Zhang & Sikdar, 2022; Edwards & Rawat, 2020). Otro ataque que pertenece al dominio de la imágen es el presentado por Quiring y Rieck (2018) quienes hacen un ataque contra esquemas de marcas de agua, utilizando un modelo sustituto basado en una Red Neuronal Profunda para engañar al detector de marcas de agua original. Este ataque, conocido como C&W, destaca por su efectividad y no requiere un conocimiento detallado del esquema de marcas de agua original.

--Procesamiento de Lenguaje Natural (NLP)

En el dominio del Procesamiento de Lenguaje Natural (NLP), los ataques adversarios presentan desafíos distintos a los observados en el procesamiento de imágenes. Marulli et al. (2021) destacan que los ataques diseñados para sistemas de imágenes no son efectivos cuando se aplican a representaciones vectoriales utilizadas en NLP. Edwards y Rawat (2020) señalan que los ejemplos adversarios en NLP pueden afectar el rendimiento del modelo al modificar semántica, ortografía o frases completas, aunque estas perturbaciones suelen ser perceptibles. Por ejemplo, Cresci et al. (2022) describen ataques contra sistemas de detección de noticias falsas, como "TextBugger", que manipulan el contenido de noticias para engañar clasificadores. Estos ataques influyen en el título, contenido o fuente de una noticia para revertir el resultado del clasificador. 

--Dominio inalámbrico 

En el ámbito inalámbrico, el aumento del uso de aplicaciones de Aprendizaje Máquina ha suscitado la necesidad de comprender los desafíos de seguridad asociados. Shi et al. (2019) señalan una falta de investigación sobre ataques de Aprendizaje Máquina Adversaria en la clasificación del tráfico de red, posiblemente debido a la complejidad de alterar los paquetes de datos sin cambiar su significado. Davaslioglu & Sagduyu (2019) presentan un ejemplo de ataque en el dominio inalámbrico, utilizando un ataque de envenenamiento tipo Backdoor para manipular el comportamiento de un modelo de Aprendizaje Máquina durante la fase de pruebas.

Por otro lado, Usama et al. (2019) y Usama, Qayyum, et al. (2019) investigan ataques adversarios en redes cognitivas autónomas y en sistemas de clasificación del tráfico TOR, respectivamente. Ambos estudios revelan que tanto los modelos de modulación basados en Aprendizaje Máquina como las técnicas de Deep Learning son vulnerables a ataques adversarios.

Además, Catak et al. (2021) destacan la importancia de la seguridad en las redes 6G y proponen el uso del "Fast Gradient Sign Method" (FGSM) para generar muestras adversarias y fortalecer la robustez de los modelos de Aprendizaje Máquina.


--Ciberseguridad

El ámbito de la ciberseguridad enfrenta desafíos únicos en relación con los ataques de Aprendizaje Máquina Adversaria (AMA). Los sistemas de Aprendizaje Máquina (ML) en seguridad informática son más complejos y dinámicos, lo que los hace más vulnerables a ataques, aunque la investigación en este campo aún es limitada (Yeboah-Ofori et al., 2021; Shi et al., 2019).

Ataques contra sistemas DNS: Jin et al. (2019) abordan los ataques de envenenamiento a servidores DNS, destacando la importancia de técnicas de defensa como el entrenamiento adversario.

Ataques contra IDS: Anthi et al. (2021) y Ayub et al. (2020) investigan ataques de AML contra modelos de detección de intrusos (IDS), revelando una notable disminución en la precisión de los modelos después de ser atacados.

Ataques contra sistemas de detección de malware y SPAM: Chen et al. (2017) describen un ataque de evasión llamado "EnvAttack" que manipula características de malware para evadir la detección. Además, Yeboah-Ofori et al. (2021) mencionan ataques de AML en sistemas de detección de SPAM, resaltando la importancia de la defensa contra estos ataques.

--Estados cuanticos y audio

Los modelos de clasificación basados en Aprendizaje Automático (ML) cuánticos ofrecen eficiencia en la clasificación de grandes volúmenes de datos, aunque enfrentan desafíos de seguridad similares a los modelos tradicionales. Edwards y Rawat (2020) advierten sobre el impacto negativo que pueden tener los ejemplos adversarios en los modelos de ML cuánticos, destacando la posibilidad de ataques como el Método del Signo de Gradiente Rápido (FGSM) y el riesgo de ataques de transferencia, aunque este último aún no se ha explorado ampliamente.

Es esencial realizar una revisión exhaustiva de las vulnerabilidades de los modelos de ML cuánticos antes de su implementación en producción, abordando las debilidades existentes para garantizar su robustez y seguridad.

En el dominio del procesamiento de audio, los ejemplos adversarios son menos explorados pero pueden tener un impacto significativo en el rendimiento de los modelos. Lin y Biggio (2021) presentan un caso de ataque adversario de evasión basado en audio dirigido al sistema de reconocimiento de voz a texto de Mozilla DeepSpeech, donde la inclusión de ruidos apenas perceptibles comprometió la precisión del modelo, resaltando la vulnerabilidad en este contexto específico.

-Perturbaciones

El concepto de perturbaciones o muestras adversarias es fundamental en el ámbito del Aprendizaje Máquina Adversaria (AML), donde los atacantes introducen datos maliciosos durante el entrenamiento y las pruebas de modelos de clasificación de Aprendizaje Máquina (ML) para engañar a estos modelos. Estas perturbaciones, casi imperceptibles para los humanos pero capaces de inducir clasificaciones incorrectas en los modelos, representan un desafío de seguridad significativo (Yeboah-Ofori et al., 2021; Ntalampiras, 2023; Usama et al., 2019; Guihai & Sikdar, 2021; McDaniel et al., 2016).

Christian Szegedy y sus colegas fueron los primeros en destacar la vulnerabilidad de las redes neuronales profundas frente a estas perturbaciones casi imperceptibles en las entradas, lo que puede llevar a predicciones erróneas en los modelos (Szegedy et al., 2013, citado por Liu et al., 2020).

Estas perturbaciones, también llamadas ejemplos adversarios, pueden ser diseñadas para maximizar el error de predicción del modelo (Usama et al., 2020; Khalid et al., 2019; Kurakin et al., 2016; Verma et al., 2022; Cresci et al., 2022; Seo et al., 2022). Pueden afectar diversos sistemas de ML, siendo los sistemas de Deep Learning especialmente vulnerables (Guihai & Sikdar, 2021; Kurakin et al., 2016; Tian et al., 2022; Liu et al., 2020; Marulli et al., 2021). La capacidad de transferibilidad de estas perturbaciones, es decir, su efectividad en diferentes modelos, es un aspecto importante a considerar (Lin & Biggio, 2021; Quiring & Rieck, 2018).

La generación de estas perturbaciones se ha convertido en un área de investigación activa, con métodos como el Fast Gradient Sign Method (FGSM) utilizado para crear perturbaciones óptimas (Davaslioglu & Sagduyu, 2019).

Además de su uso en la generación de ataques, las perturbaciones adversarias también tienen aplicaciones en la ofuscación de modelos de ML, ayudando a proteger los modelos contra ataques maliciosos (G. Verma et al., 2018).

-Taxonomia 

Dentro del campo del Aprendizaje Máquina Adversaria (AML), varias propuestas de clasificación han surgido para categorizar las técnicas de ataques:
	-Taxonomía del NITS: Desarrollada por el Instituto Nacional de Estándares y Tecnología (NITS), divide los ataques en tres categorías principales: objetivos, técnicas y conocimiento (Tabassi et al., 2019).
	-Propuesta de Barreño: Organiza los ataques en tres ejes: influencia, violación de seguridad y especificidad (Barreno et al., 2006).
	-Categorización de Papernot: Considera la complejidad del ataque y el conocimiento del adversario sobre el modelo atacado (Papernot et al., 2016).
	-Taxonomía de Khalid et al.: Clasifica los ataques en tres categorías principales según el momento y método de ataque: entrenamiento, inferencia e implementación de hardware (Khalid et al., 2019).
	-Clasificación de Olney & Karam: Distingue entre ataques causales, dirigidos a subvertir el proceso de entrenamiento, y exploratorios, destinados a exponer información sobre la arquitectura subyacente del modelo (Olney & Karam, 2022).

Otras formas de clasificar los ataques de AML son:
	1.Ataques según el tipo de violación de seguridad:
		-Ataques de confiabilidad: Buscan maximizar el error de predicción del modelo, haciéndolo inútil, especialmente crítico en aplicaciones como seguridad y atención médica (Ma et al., 2020).
		-Ataques de integridad: Afectan la precisión del modelo al causar clasificaciones incorrectas y altas tasas de error, comunes en entornos adversarios durante la operación de sistemas de ML (Anthi et al., 2021).
		-Ataques de disponibilidad: Pretenden impedir la utilidad del sistema, induciendo errores en el modelo o denegando servicios, siendo conocidos también como ataques de envenenamiento (Mehta et al., 2022).
	2.Ataques según la especificidad del ataque:
		-Ataques dirigidos: Tienen un objetivo específico en mente, como cambiar la predicción del modelo a una salida fija y alternativa en un conjunto de datos específico (Mehta et al., 2022).
		-Ataques indiscriminados: Carecen de un objetivo específico y buscan alterar las clasificaciones de manera aleatoria (Liu et al., 2020).
		-Ataques combinados: Algunas metodologías, como FaDec propuesta por Khalid et al. (2020), pueden fusionar elementos de ataques dirigidos y no dirigidos para maximizar la eficiencia del ataque.
	4.Ataques en relación al impacto en el proceso de aprendizaje:
		-Ataques en Fase de Pruebas: Conocidos como ataques exploratorios, buscan manipular datos "no etiquetados" para evitar la detección durante la etapa de pruebas o inferencia (Ma et al., 2020).
		-Ataques en Fase de Entrenamiento: También llamados ataques causales, subvierten el proceso de entrenamiento al introducir ejemplos adversarios en el conjunto de datos de entrenamiento (Olney & Karam, 2022).
	5.Ataques en relación a la información (conocimiento) del adversario:
		-Ataques de caja blanca: El adversario tiene acceso completo a la información del modelo, facilitando la generación de perturbaciones adversarias (Ma et al., 2020).
		-Ataques de caja negra: El adversario carece de información sobre el modelo, construyendo un modelo sustituto para generar el ataque y aprovechando la transferibilidad (Verma et al., 2022).
		-Ataques de caja gris: El adversario opera con información parcial sobre el modelo, buscando influir en las fases de entrenamiento y prueba (Ayub et al., 2020).

Ataques de evasión

Los ataques de evasión, conocidos como Evassion Attacks, ocurren durante la fase de pruebas de un modelo de Aprendizaje Máquina. El objetivo es manipular los datos "NO ETIQUETADOS" para evitar la detección durante la etapa de prueba, sin alterar el proceso de entrenamiento (Ma et al., 2020; Olney & Karam, 2022; Venkatesan et al., 2021; Chen et al., 2017; Ebrahimabadi et al., 2021; Khalid et al., 2019; Zizzo et al., 2019; Catak et al., 2021; Anthi et al., 2021; Sun et al., 2022; Olney & Karam, 2022). Esto implica agregar ruido cuidadosamente a las instancias de los datos de prueba para crear perturbaciones adversarias (Ma et al., 2020; Lin & Biggio, 2021; Khalid et al., 2019), lo que permite evadir la capacidad del modelo existente (Hao & Tao, 2022). Estas perturbaciones pueden hacer que los modelos clasifiquen erróneamente las instancias como legítimas, lo que facilita ataques dirigidos o indiscriminados (F. O. Catak et al., 2022; Quiring & Rieck, 2018; Mehta et al., 2022). Ampliamente estudiados, estos ataques representan una amenaza importante, ya que pueden duplicar la probabilidad de error y no requieren modificar la estructura del modelo (Olney & Karam, 2022; Venkatesan et al., 2021). Este tipo de ataques se basan en gradientes y son más poderosos que los ataques sin gradientes, como el Fast Gradient Sign Method, el Iterative Gradient Sign o el ataque de C&W (Zizzo et al., 2019). Un ejemplo es el descrito por Verma et al. (2022), donde se alteran muestras de malware para evitar la detección. Catak et al. (2021) señalan que estos ataques se utilizan en casos de phishing, spam y análisis de malware. Además, W. Li et al. (2022) detallan un ataque que engaña a un clasificador SVM, técnica aplicable a SVM lineales según Frederickson et al. (2018). Estos ataques, también llamados ataques exploratorios, buscan exponer la arquitectura del modelo al proporcionar ejemplos contradictorios (Frederickson et al., 2018; Olney & Karam, 2022).		

Lista de ataques de evasión 

	Fast Gradient Sign Method (FGSM): El Fast Gradient Sign Method (FGSM), propuesto por Ian Goodfellow, busca aproximar linealmente la función de costo cerca de las muestras legítimas para generar rápidamente perturbaciones adversarias (Goodfellow et al., 2014, citado por Verma et al., 202; Goodfellow et al., 2014, citado por Liu et al., 2020). Este método se basa en el uso de gradientes para calcular perturbaciones que aumenten la función de pérdida y evadan la detección del modelo (Usama, Qayyum, et al., 2019; Usama et al., 2020; Usama et al., 2019; Kurakin et al., 2016; F. O. Catak et al., 2022; Catak et al., 2021). FGSM puede realizar ataques dirigidos y no dirigidos, controlando la norma L1, L2 o L∞ de la alteración (Ntalampiras, 2023). Aunque es simple y eficiente computacionalmente, se señalan debilidades, como su falta de robustez y su desempeño deficiente con gradientes pequeños (Zizzo et al., 2019; Usama et al., 2019; Mehta et al., 2022). Existen variantes del FGSM, como el Fast Gradient Value (FGV), que maximiza la pérdida en un solo paso (Guihai & Sikdar, 2021), y el Iterative Gradient Sign (IFGS), que agrega perturbaciones en múltiples iteraciones (Liu et al., 2020; Zizzo et al., 2019). Además, está el Projected Gradient Descent (PGD), una versión iterativa del FGSM que utiliza el algoritmo de retropropagación (Zhang & Sikdar, 2022). También se menciona el Basic Iterative Method (BIM), que aplica un método iterativo de optimización para generar perturbaciones (Usama et al., 2018, citado por Usama et al., 2020; F. O. Catak et al., 2022). Una variante del BIM es el Momentum Iterative Method (MIM), que introduce un impulso en el proceso (F. O. Catak et al., 2022). Finalmente, el Iterative Least-Likely Class Method, utilizado contra el modelo "Inception", demuestra su superioridad sobre las defensas adversarias (Kurakin et al., 2016).

	Ataque Jacobian-based Saliency Map Attack JSMA: El Jacobian-based Saliency Map Attack (JSMA), creado por Nicolas Papernot y colaboradores, es una estrategia iterativa para generar muestras adversarias, utilizando el jacobiano del modelo para determinar las perturbaciones necesarias para lograr una clasificación específica por parte del atacante (Papernot et al., p. 372-387 2016, citado por McDaniel et al., 2016; Papernot et al., p. 372-387 2016, citado por Usama, Qayyum, et al., 2019; Papernot et al., p. 372-387 2016, citado por Usama et al., 2020). Mehta et al. (2022) destacan que, en el contexto de un clasificador de imágenes, este enfoque manipula los píxeles de una imagen, saturándolos con sus valores máximos y mínimos para inducir una clasificación errónea. Además, Verma et al. (2022) presentan una aplicación en la que se utiliza el ataque JSMA en un sistema de clasificación de malware, demostrando los desafíos adicionales que presenta este tipo de clasificación debido a la naturaleza de los archivos PE. Otro caso ilustrativo es presentado por Ayub et al. (2020), donde el ataque JSMA se utiliza en sistemas basados en Aprendizaje Automático para la detección de intrusos (IDS), generando muestras de datos que confunden al modelo entrenado para clasificar datos maliciosos como benignos.

	DeepFool: El ataque conocido como DeepFool, propuesto por Moosavi-Dezfooli y colegas (Moosavi-Dezfooli et al., 2016, citado por Usama et al., 2019), tiene como objetivo evadir clasificadores de Aprendizaje Máquina mediante la generación iterativa de perturbaciones adversarias al linearizar el clasificador. Este método busca actualizar la muestra original hasta que esté ligeramente más allá del hiperplano de clasificación. Sin embargo, su capacidad de transferibilidad es limitada.

	L-BFGS: El ataque Broyden-Fletcher-Goldfarb-Shannon (L-BFGS), ideado por Szegedy (Szegedy et al., 2013, citado por Verma et al., 2022) y aplicado por Verma et al. (2018), se centra en la detección de perturbaciones adversarias en Redes Neuronales Profundas para ofuscar las muestras y agregar defensa al modelo.

	C&W: El ataque de C&W, basado en L-BFGS, es un enfoque iterativo para generar perturbaciones adversarias óptimas que optimizan la clasificación errónea mientras minimizan la distorsión introducida en las muestras (Xi, 2020, citado por Mehta et al., 2022). Este método ha demostrado ser efectivo en la elusión de clasificadores, incluso mostrando prometedores resultados en la reentrenamiento del modelo.

	EnvAttack: El ataque EnvAttack, presentado por Chen et al. (2017), manipula un subconjunto de características con un costo mínimo para evadir la detección, empleando un enfoque de envoltura para seleccionar características de manera efectiva.

	Mutual Information: El método Mutual Information, descrito por Usama et al. (2020), perturba el tráfico anómalo mientras garantiza el funcionamiento normal del sistema, utilizando el concepto de información mutua para identificar características distintivas y reducir la diferencia entre ellas.

	SIFA: El ataque Simple Iterative FDI Attack (SIFA) de Guihai y Sikdar (2021), dirigido a sistemas de detección de ataques FDI basados en Deep Learning, utiliza predicciones de demanda normales y perturbaciones calculadas mediante descenso de gradiente para evadir técnicas de detección y tener éxito en ataques de "Demanda Distribuida".

Ataques de Envenenamiento

Los ataques de envenenamiento, conocidos como "Model poisoning", tienen como objetivo principal introducir instancias con ruido y modificar las etiquetas de las muestras existentes durante la etapa de entrenamiento de un modelo (Ma et al., 2020; Verma et al., 2022; Lin & Biggio, 2021; Olney & Karam, 2022; Hao & Tao, 2022; Venkatesan et al., 2021; Marulli et al., 2021; Chiba et al., 2020; Tian et al., 2022; Baracaldo et al., 2018; Davaslioglu & Sagduyu, 2019). Estos ataques también se conocen como "ataques causales" (Sun et al., 2022; Shi et al., 2019; Frederickson et al., 2018; Olney & Karam, 2022), y pueden tener como finalidad la manipulación de un modelo para maximizar el error de clasificación y negar el servicio (Khalid et al., 2019). La inyección de muestras envenenadas cerca de datos de alta confianza afecta significativamente el proceso de entrenamiento, propagando eficazmente las etiquetas de las muestras envenenadas y resultando en una degradación del rendimiento del modelo resultante (Venkatesan et al., 2021).

Los ataques de envenenamiento son efectivos contra una variedad de algoritmos, incluidos los de Support Vector Machine (SVM) y Deep Learning (Frederickson et al., 2018), y han sido identificados como una preocupación principal para las empresas que implementan sistemas basados en Aprendizaje Máquina (Venkatesan et al., 2021). Estos ataques tienen el potencial de afectar diversas áreas más allá de las aplicaciones específicas existentes, como la visión por computadora, y pueden comprometer la integridad y disponibilidad de los modelos (Davaslioglu & Sagduyu, 2019). Los ataques de "Backdoor" son un componente crítico de los ataques de envenenamiento, capaces de reducir el rendimiento de un clasificador al inducir clasificaciones erróneas específicas o comportamientos incorrectos (Baracaldo et al., 2018). Los "troyanos neuronales" también son mencionados por Olney & Karam (2022) como capaces de alterar drásticamente el comportamiento y el rendimiento de una Red Neuronal Convolucional (CNN) mediante la modificación de datos de entrenamiento durante el proceso de capacitación.

Estos ataques también exhiben la propiedad de transferibilidad, lo que significa que las muestras que afectan a un modelo sustituto pueden transferirse y perjudicar al modelo objetivo (Chiba et al., 2020; Hao & Tao, 2022).


Lista de ataques de envenenamiento
	Ataque de Boling Frog: El Boling Frog es un tipo de ataque sofisticado que busca insertar gradualmente instancias envenenadas en un sistema. Su objetivo principal es eludir la detección y, secundariamente, influir en el rendimiento del clasificador subyacente. Se caracteriza por su enfoque sutil y estratégico en la introducción de instancias maliciosas (Lagesse et al., 2016; Ma et al., 2020).

	Outler detection evasion term: Frederickson et al. (2018) emplean una estrategia propuesta por Xiao y sus colegas, llamada "Poissoning Embedded Feature Selection", para crear un único punto de ataque dirigido a algoritmos de selección de características integradas, como Lasso. Esta estrategia se basa en el algoritmo de ascenso de gradiente con un tamaño de paso constante para perturbar la dirección de optimización y mantener efectivo el punto de ataque. La innovación de Frederickson et al. radica en la incorporación de un "Término de Penalización" en la función objetivo del atacante, que permite generar puntos de ataque que maximizan el impacto en la función objetivo del aprendiz y minimizan la probabilidad de ser detectados por el defensor. Los experimentos realizados demuestran que esta modificación permite crear instancias de ataque capaces de evadir la detección de valores atípicos en clasificadores entrenados en conjuntos de datos del mundo real.

	ATTack on Federated Learning (AT2FL): Sun et al. (2022) proponen el "ATTack on Federated Learning" (AT2FL) como respuesta al riesgo de ataques de envenenamiento de datos en nodos de Federated Learning. Este enfoque de optimización consciente del sistema eficientemente deriva gradientes implícitos de los datos envenenados para lograr estrategias de ataque óptimas en este contexto. AT2FL formula el envenenamiento de datos como un problema de optimización de dos niveles adaptable a diversas configuraciones de nodos. Los experimentos con un Modelo de Federated Machine Learning entrenado con datos reales confirman el impacto significativo del ataque en todos los conjuntos de datos, validando la vulnerabilidad del modelo.

	Ataque Adversarial Label Flips (LFA): Olney & Karam (2022) describen el "Ataque Adversarial Label Flips" (LFA), concebido por Huang Xiao y Claudia Eckert en 2012, como una táctica de envenenamiento que ha sido objeto de estudio detallado. Este ataque busca minimizar la cantidad de muestras contaminadas mientras maximiza la tasa de errores de clasificación en el modelo objetivo. No necesita un disparador para activarse, lo que lo convierte en un riesgo persistente. Los autores utilizan una versión ambigua del LFA para simular aumentos en la tasa de error y cambios en los parámetros de una red durante la fase de reentrenamiento, destacando la importancia de la detección temprana y la protección contra estas amenazas en el aprendizaje automático.

	False Data Injection Attack: Tian et al. (2022) proponen un método que aprovecha los ataques de inyección de datos falsos para crear perturbaciones adversarias sutiles y de pequeño tamaño, evitando la detección al agregar perturbaciones a variables de estado no atacadas. Sus resultados revelan la efectividad de estos ataques en eludir métodos de detección de datos incorrectos y de ataques neuronales, mostrando la vulnerabilidad de estos métodos en esta situación.

	Ataque Binary-Search (Búsqueda Binaria): El "Ataque Binary-Search" (Búsqueda Binaria), según detallado por Ma et al. (2020), se basa en considerar la instancia objetivo como un valor atípico con respecto a los datos de entrenamiento de la clase opuesta. Su objetivo principal es generar instancias envenenadas que conecten el objetivo con la clase deseada para atenuar la influencia de la clase objetivo. En cada iteración, el ataque utiliza un punto medio entre la muestra objetivo y su vecino más cercano en la clase opuesta como candidato de envenenamiento. Si pertenece a la clase deseada, se agrega al conjunto de datos de entrenamiento y se ajusta el límite de clasificación gradualmente hacia el objetivo hasta invertir su etiqueta. Si no cumple con esta premisa, se reinicia el proceso.

	Ataque StingRay (Ataque de mantarraya): El "Ataque StingRay" (Ataque de mantarraya) según Ma et al. (2020), introduce nuevas copias de instancias al perturbar características menos críticas. Similar al ataque Binary-Search, selecciona una instancia base cercana al objetivo en la clase deseada, crea una copia de esta instancia y la perturba. La instancia envenenada más cercana al objetivo se introduce en los datos de entrenamiento.

	Adversarial Attack on RF & GBoost Classifiers: Yeboah-Ofori et al. (2021) proponen un ataque adversario dirigido a clasificadores Random Forest y Gradient Boosting. El ataque se basa en desplazar puntos dentro de las muestras más allá del hiperplano más cercano para inducir perturbaciones y posibles clasificaciones incorrectas. Los experimentos se enfocan en predecir un ataque de "ransomware", alterando la detección de intrusiones en la red mediante la inserción maliciosa de una entrada en las funciones, comprometiendo así el rendimiento del modelo.

	Gradient ascent y High confidence data poisoning: Venkatesan et al. (2021) llevaron a cabo experimentos sobre la manipulación de modelos de Aprendizaje Máquina utilizados en la detección de intrusiones en redes (NIDS). Exploraron dos tipos de ataques de envenenamiento. El primero, llamado "Gradient ascent", ajusta un punto en la dirección de una clase objetivo mediante el cálculo del gradiente de la función de pérdida. Aunque provocó errores significativos en los conjuntos de validación y prueba, sus gradientes convergieron rápidamente. El segundo ataque, "High confidence data poisoning", expande el conjunto de datos introduciendo perturbaciones cerca de muestras de alta confianza, añadiendo etiquetas que se asemejan a estas, lo que afecta significativamente el proceso de entrenamiento.

	Ataques Backdoor y Troyanos neuronales: Los ataques de puerta trasera (Backdoors) son una táctica fundamental en el Aprendizaje Máquina Adversario, donde se insertan patrones especiales, llamados "disparadores", durante el entrenamiento del modelo para activarse más tarde y desencadenar un comportamiento específico (Lin & Biggio, 2021; Marulli et al., 2021; Chiba et al., 2020; Olney & Karam, 2022). Estos ataques pueden manipular la canalización de entrenamiento al introducir muestras contaminadas (Davaslioglu & Sagduyu, 2019; Venkatesan et al., 2021), lo que plantea preocupaciones de seguridad, especialmente en datos de código abierto (Lin & Biggio, 2021). Los "troyanos neuronales" son una variante de estos ataques, difíciles de detectar ya que solo se activan con entradas específicas conocidas por el atacante (Olney & Karam, 2022). Se introducen durante el entrenamiento y pueden afectar la clasificación en la fase de pruebas (Davaslioglu & Sagduyu, 2019). Un ejemplo específico incluye la inserción de palabras envenenadas en un modelo de Reconocimiento de Entidades Nombradas (NER) (Marulli et al., 2021). Estos ataques, dirigidos a datos públicos en línea, representan una amenaza emergente (Davaslioglu & Sagduyu, 2019).

Ataques adversarios de modelado/transferencia

Las perturbaciones adversarias exhiben transferibilidad, lo que significa que pueden afectar a modelos similares incluso con diferentes arquitecturas y datos de entrenamiento (Lin & Biggio, 2021; Usama et al., 2020; Verma et al., 2022; Olney & Karam, 2022). Papernot et al. (2016, citado por Hao & Tao, 2022) introdujeron la noción de transferibilidad intratecnológica y entre diferentes tipos de modelos. Esta característica es explotada en ataques de caja gris y negra (Verma et al., 2022).

Estos ataques, también llamados ataques de transferencia, extracción o robo de modelos, aprovechan esta propiedad (Edwards & Rawat, 2020; Quiring & Rieck, 2018; Lin & Biggio, 2021). Por ejemplo, Zhang y Sikdar (2022) presentan ETAA, un método que mejora la capacidad de transferencia de ataques adversarios entre distintos modelos de Deep Learning. ETAA opera en dos etapas, simulando un ataque de caja blanca en un conjunto de modelos y luego eligiendo un modelo para atacar como caja negra, lo que mejora la transferibilidad de las perturbaciones.

Quiring y Rieck (2018) abordan el aprendizaje de un modelo sustituto para eliminar marcas de agua en señales. Después de encontrar un modelo sustituto, aplican un descenso de gradiente para suprimir las marcas de agua en la señal marcada, lo que permite eludir la detección por el clasificador original. Este proceso se realiza ajustando la formulación de C&W y aplicando un descenso de gradiente hasta que el modelo sustituto predice la ausencia de la marca de agua con alta confianza.